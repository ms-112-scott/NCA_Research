# E4-5.x Overfitting 困難分析與診斷

## 1. 問題概述

在 `E4-5.0_UrbanTales_GNCA_overfit.ipynb` 和 `E4-5.1_UrbanTales_GNCA_overfit.ipynb` 兩個實驗中，我們觀察到模型（無論是純資料驅動的 GNCA 還是整合物理定律的 PINN-NCA）都無法在單一的 UrbanTales 資料樣本上達成過擬合 (Overfitting)。Loss 在初期下降後便停滯在一個相對較高的水平，無法收斂到接近零，這表明訓練過程中存在著根本性的阻礙。

本分析旨在深入探討您提出的三個潛在問題，並提供具體的證據和解決建議。

## 2. 診斷分析

### 2.1 Dataset HW Resize 後失去物理意義（關鍵問題）

**診斷：** 這是導致 PINN-NCA (`E4-5.1`) 訓練失敗的**最核心問題**。

**證據：**
1.  **資料前處理：** 在 `E4-4.0.1_Data_Preprocessing_UrbanTales_BCHW.ipynb` 中，原始資料 (`(240, 360)`) 被 `torch.nn.functional.interpolate` 函數 resize 成了 `(64, 64)`。
2.  **導數計算方式：** 在 `E4-5.1` 的 `pinn_loss` 函數中，物理損失（連續性方程 `L_cont` 和動量方程 `L_mom`）是透過**固定大小的卷積核**（如 Sobel, Laplacian）來近似偏導數的。
3.  **物理矛盾：** 這些卷積核隱含了一個假設，即網格間距 `dx` 和 `dy` 為 1。但當圖像從 `(240, 360)` 被縮放到 `(64, 64)` 時，網格的物理尺寸已經發生了巨大變化。在一個被扭曲的網格上使用固定的導數算子，會產生完全錯誤的物理殘差 (physical residuals)。

**結論：** 模型在 `E4-5.1` 中被要求同時最小化兩個相互矛盾的目標：
*   **Data Loss (`L_data`)**: 讓輸出接近被 resize 後的 (不準確的) Ground Truth。
*   **Physics Loss (`L_phys`)**: 滿足在錯誤網格上計算出的 (不正確的) 物理定律。
這兩個目標的衝突導致梯度方向混亂，Loss 自然會卡在一個兩者都無法滿足的局部最小值。

對於純資料驅動的 `E4-5.0`，雖然沒有物理損失的直接衝突，但 resize 後的資料可能引入了不自然的 artifacts（插值瑕疵），或丢失了重要的細節，增加了模型學習的難度。

### 2.2 Loss 定義與 PINN 限制不夠好

**診斷：** 這個觀點非常正確，是問題的延伸。目前的 Loss 設計加劇了前述的物理矛盾。

**證據：**
1.  **Loss 的量級 (Magnitude)：** 在 `E4-5.1` 的 `pinn_loss` 中，`L_cont` 和 `L_mom` 被除以了 `1e-2` 和 `1e-4` 這樣的小常數。這會人為地將物理損失放大 100 到 10000 倍。當這個被放大的、且計算不準確的物理損失被加入總 Loss 中時，它會完全主導梯度下降的方向，使得模型極度難以（甚至不可能）去擬合真實的資料 (`L_data`)。
2.  **隨機的 Loss 權重：** 實驗中，`loss_weights` 是從一個列表中隨機選取的。在尋找最佳 Loss 平衡策略的階段，更建議手動設定並一次只改變一個變量，以分析其效果。

**結論：** 不恰當的 Loss 加權策略，疊加在前述的物理計算錯誤之上，是導致模型訓練失敗的直接原因。

### 2.3 模型本身容量不足問題

**診斷：** 模型容量不足**可能不是**當前最主要的問題。

**證據：**
1.  **模型複雜度：** `CAModel` 的架構（多層 1x1 Conv，256 個隱藏單元）對於 overfit 一個僅 `64x64` 的單一樣本來說，理論上容量是足夠的。
2.  **問題的普遍性：** `E4-5.0`（純資料）和 `E4-5.1`（PINN）都失敗了。如果僅是容量問題，通常更複雜的模型 (`E4-5.1` 的 Loss 更複雜) 可能會表現出不同的行為。兩者相似的失敗模式，更指向一個共通的問題——即資料本身（被 resize）和訓練動態（Loss 設計）。

**結論：** 在解決了資料保真度和 Loss 設計這兩個更根本的問題之後，如果模型依然無法 overfit，才需要回頭深入探討模型架構的容量或更新規則的有效性。

## 3. 後續處理建議 (基於您的規劃)

您的後續處理規劃非常合理，完全切中了要害。我在此基礎上提供更詳細的執行建議：

1.  **檢視 Dataset (最優先)：**
    *   **建議：** 停止使用 `interpolate` 進行 resize。直接在原始資料上进行 **patch-based learning**。例如，從 `(240, 360)` 的高解析度資料中隨機裁切出 `64x64` 的 patch 進行訓練。
    *   **優點：** 這樣既能保持原始的物理尺度（`dx`, `dy` 不變），又能利用高解析度的細節，同時还能透過隨機裁切起到資料增強的作用。
    *   **去量綱化：** 在此階段可以暫緩。首先要確保模型能在一个物理保真的 patch 上 overfit。成功後，再去考慮特徵工程如去量綱化，以提升模型的泛化能力。

2.  **Loss PINN 設計 (同步進行)：**
    *   **建議：** 暫時**移除** `L_cont` 和 `L_mom` 中手動放大的除數（如 `/1e-2`, `/1e-4`）。
    *   **策略：** 從一個極小的物理損失權重 `λ_phys` 開始（例如 `1e-4` 或更小），主要讓模型先學習擬合資料 (`L_data`)。在 `L_data` 下降到一定程度後，再逐步增長 `λ_phys`（一種 curriculum learning 的思想），引導模型去遵守物理定律。

3.  **套用 Dataset Target 查看原始 Loss 數值 (關鍵診斷步驟)：**
    *   **建議：** 在開始訓練前，單獨執行一次 Loss 計算。將 Ground Truth `Y` 同時作為 `x` (模型輸出) 和 `y` (目標) 傳入 `pinn_loss` 函數。
    *   **目的：**
        *   `L_data` 此時應為 0。
        *   `L_cont` 和 `L_mom` 的值則代表了 Ground Truth 資料本身與理想物理定律的偏差（或由數值模擬、資料處理引入的誤差）。
    *   **分析：** 這一步可以讓您了解資料的“乾淨”程度，以及物理損失的初始量級，為後續的 Loss 加權提供重要參考。

4.  **Patch Learning：**
    *   如第 1 點所述，這應是解決資料保真度問題的核心方案。請將其作為首要步驟。

**總結：** 當前的核心矛盾是**在被錯誤縮放的網格上，強制模型學習被錯誤計算的物理定律**。解決方案是回歸到物理保真的資料（通过 Patch Learning），並重新設計一套更合理的 Loss 加權與訓練策略。