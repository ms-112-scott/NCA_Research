{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b343a06",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ad1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from typing import Tuple\n",
    "import os\n",
    "\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/GAI/Desktop/Scott/NCA_Research\")\n",
    "\n",
    "\n",
    "from core_utils.plotting import *\n",
    "from E4_PI_NCA.utils.helper import *\n",
    "\n",
    "\n",
    "from core_utils.utils_image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9238af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_h5_structure(obj, indent=0):\n",
    "    \"\"\"\n",
    "    éžè¿´åˆ—å° h5 å…§å®¹\n",
    "    - dataset: print shape\n",
    "    - group: éžè¿´\n",
    "    - attrs: print key/values\n",
    "    - list/dict/tuple: print len()\n",
    "    \"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    if isinstance(obj, h5py.File) or isinstance(obj, h5py.Group):\n",
    "        print(f\"{prefix}{obj.name} (Group)\")\n",
    "        # åˆ—å° attributes\n",
    "        for k, v in obj.attrs.items():\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                print(f\"{prefix}  attr {k}: {type(v).__name__}, len={len(v)}\")\n",
    "            else:\n",
    "                print(f\"{prefix}  attr {k}: {type(v).__name__} -> {v}\")\n",
    "        # éžè¿´åˆ—å°å­ç‰©ä»¶\n",
    "        for key in obj:\n",
    "            print_h5_structure(obj[key], indent + 1)\n",
    "    elif isinstance(obj, h5py.Dataset):\n",
    "        print(f\"{prefix}{obj.name} (Dataset) shape={obj.shape} dtype={obj.dtype}\")\n",
    "    else:\n",
    "        print(f\"{prefix}{obj.name}: Unknown type {type(obj)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a5f3f",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5001e69",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "å°‡ dataset/ ä¸‹çš„æ‰€æœ‰ {case}/ è³‡æ–™å¤¾è½‰æ›æˆåˆ†åŸŽå¸‚çš„ HDF5 çµæ§‹ã€‚\n",
    "\n",
    "æ¯å€‹ city å°æ‡‰ä¸€å€‹ .h5 æª”æ¡ˆï¼Œçµæ§‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "GlobalMetaData.h5\n",
    "\n",
    "Cities/\n",
    "\n",
    "    city_X.h5\n",
    "\n",
    "        â”œâ”€ city_base (C,H,W)\n",
    "\n",
    "        â””â”€ cases/\n",
    "\n",
    "            â”œâ”€ case_0/wind_field (C,H,W)\n",
    "\n",
    "            â”œâ”€ case_0/meta (å±¬æ€§)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f709a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_files(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                # è‹¥è¦åŒæ™‚åˆªé™¤å­è³‡æ–™å¤¾å…§çš„å…§å®¹\n",
    "                remove_all_files(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c45985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Helper Functions ----------------------\n",
    "#region Helper Functions\n",
    "def nan_to_masked_CHW(arr_CHW: np.ndarray):\n",
    "    nan_mask = np.isnan(arr_CHW).any(axis=0)\n",
    "    geo_mask = (~nan_mask).astype(np.float16)\n",
    "    geo_mask = geo_mask[::-1]\n",
    "    arr_clean = np.array(arr_CHW, copy=True)\n",
    "    arr_clean[np.isnan(arr_clean)] = 0.0\n",
    "    return arr_clean, geo_mask\n",
    "\n",
    "\n",
    "def add_coord_channels(arr_CHW: np.ndarray, geo_mask: np.ndarray):\n",
    "    C, H, W = arr_CHW.shape\n",
    "    ys = np.linspace(-1.0, 1.0, H, dtype=np.float16)[:, None]\n",
    "    xs = np.linspace(-1.0, 1.0, W, dtype=np.float16)[None, :]\n",
    "    coord_y = np.repeat(ys, W, axis=1)\n",
    "    coord_x = np.repeat(xs, H, axis=0)\n",
    "    stacked = np.concatenate([\n",
    "        coord_y[np.newaxis, ...],\n",
    "        coord_x[np.newaxis, ...],\n",
    "        geo_mask[np.newaxis, ...],\n",
    "        arr_CHW\n",
    "    ], axis=0)\n",
    "    return stacked\n",
    "\n",
    "\n",
    "\n",
    "def extract_city_name_and_wind(case_name: str) -> Tuple[str, float, float]:\n",
    "    \"\"\"\n",
    "    å¾ž case_name æå– city åç¨±èˆ‡é¢¨å‘è³‡è¨Š\n",
    "    é¢¨å‘è§’åº¦ä»¥åŒ—ç‚º 0 åº¦ï¼Œé€†æ™‚é‡ï¼Œå–®ä½ degree\n",
    "    å›žå‚³ï¼š\n",
    "        city_name: str\n",
    "        wind_dir_deg: float\n",
    "        wind_xy: Tuple[float, float] (x, y)\n",
    "    ç¯„ä¾‹ï¼š\n",
    "        case_name = \"CN-BE-V1_d150\"\n",
    "        -> city_name=\"CN-BE-V1\", wind_dir_deg=150, wind_xy=(sin, cos)\n",
    "    \"\"\"\n",
    "    parts = case_name.split(\"_\")\n",
    "    city_name = parts[0] if len(parts) > 1 else \"city_default\"\n",
    "\n",
    "    # å˜—è©¦æå– _dXX\n",
    "    wind_dir_deg = 0.0\n",
    "    for p in parts:\n",
    "        if p.startswith(\"d\"):\n",
    "            try:\n",
    "                wind_dir_deg = float(p[1:])\n",
    "            except:\n",
    "                wind_dir_deg = 0.0\n",
    "            break\n",
    "\n",
    "    # è½‰æ›æˆ xy åˆ†é‡ (x:æ±, y:åŒ—)\n",
    "    # é¢¨å‘è§’ä»¥åŒ—ç‚º 0ï¼Œé€†æ™‚é‡\n",
    "    wind_rad = math.radians(wind_dir_deg)\n",
    "    wind_x = math.sin(wind_rad)\n",
    "    wind_y = math.cos(wind_rad)\n",
    "\n",
    "    return city_name, wind_dir_deg, (wind_x, wind_y)\n",
    "\n",
    "\n",
    "def process_ped_file(ped_file: Path):\n",
    "    \"\"\"\n",
    "    è™•ç† ped.ncï¼Œå›žå‚³ï¼š\n",
    "    - city_base_channels: å›ºå®šé€šé“ (coord_y, coord_x, geo_mask, topo)\n",
    "    - case_channels: å‰©ä¸‹çš„é€šé“ (windInitX, windInitY, ped data)\n",
    "    - city_base_channel_names\n",
    "    - case_channel_names\n",
    "    \"\"\"\n",
    "    case_name = ped_file.parent.name\n",
    "    # å˜—è©¦è§£æžé¢¨å‘\n",
    "    try:\n",
    "        wind_dir = float(case_name.split(\"_d\")[-1])\n",
    "    except Exception:\n",
    "        wind_dir = 0.0\n",
    "\n",
    "    # è®€å– ped.nc -> HWC\n",
    "    with xr.open_dataset(ped_file) as ds:\n",
    "        arrays = [ds[var].values for var in ds.data_vars]\n",
    "        ped_np = np.stack(arrays, axis=-1)  # (H,W,C)\n",
    "        vars_names = list(ds.data_vars.keys())\n",
    "\n",
    "    H, W, _ = ped_np.shape\n",
    "\n",
    "    # åˆå§‹é¢¨å ´ (H,W,2)\n",
    "    wind_np = np.zeros((H, W, 2), dtype=np.float32)\n",
    "    wind_np[..., 0] = np.sin(np.deg2rad(wind_dir))\n",
    "    wind_np[..., 1] = np.cos(np.deg2rad(wind_dir))\n",
    "\n",
    "    # topo\n",
    "    topo_file = next(ped_file.parent.glob(\"*_topo\"), None)\n",
    "    if topo_file:\n",
    "        topo = np.loadtxt(topo_file)\n",
    "        if topo.ndim == 2:\n",
    "            topo = topo[:, :, np.newaxis]\n",
    "        ped_np = np.concatenate([topo, wind_np, ped_np], axis=-1)\n",
    "        channel_names = [\"topo\", \"windInitX\", \"windInitY\"] + vars_names\n",
    "    else:\n",
    "        ped_np = np.concatenate([wind_np, ped_np], axis=-1)\n",
    "        channel_names = [\"windInitX\", \"windInitY\"] + vars_names\n",
    "\n",
    "    # nan -> mask\n",
    "    ped_np, geo_mask = nan_to_masked_CHW(ped_np.transpose(2,0,1))  # CHW\n",
    "    # åŠ å…¥ coord channels -> CHW\n",
    "    ped_np = add_coord_channels(ped_np, geo_mask)\n",
    "\n",
    "    # æ–°å¢ž channel names\n",
    "    channel_names = [\"coord_y\", \"coord_x\", \"geo_mask\"] + channel_names\n",
    "\n",
    "    # æ‹†åˆ† city_base èˆ‡ case_channels\n",
    "    city_base_idx = [\"coord_y\", \"coord_x\", \"geo_mask\", \"topo\"]\n",
    "    city_base_channels = []\n",
    "    case_channels = []\n",
    "    city_base_channel_names = []\n",
    "    case_channel_names = []\n",
    "\n",
    "    for i, name in enumerate(channel_names):\n",
    "        if name in city_base_idx:\n",
    "            city_base_channels.append(ped_np[i])\n",
    "            city_base_channel_names.append(name)\n",
    "        else:\n",
    "            case_channels.append(ped_np[i])\n",
    "            case_channel_names.append(name)\n",
    "\n",
    "    city_base = np.stack(city_base_channels, axis=0)  # CHW\n",
    "    case_data = np.stack(case_channels, axis=0)       # CHW\n",
    "\n",
    "    return city_base.astype(np.float16), case_data.astype(np.float16), city_base_channel_names, case_channel_names,(H,W)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5875ec8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "dataset_h5/\n",
    "â”œâ”€ train/\n",
    "â”‚  â”œâ”€ CityA.h5\n",
    "â”‚  â”‚  â”œâ”€ city_base                       (Dataset)\n",
    "â”‚  â”‚  â”‚    â”œâ”€ shape: (H, W, C)           # H x W x channels (HWC)\n",
    "â”‚  â”‚  â”‚    â”œâ”€ dtype: float16\n",
    "â”‚  â”‚  â”‚    â””â”€ compression: gzip\n",
    "â”‚  â”‚  â””â”€ cases                            (Group)\n",
    "â”‚  â”‚     â”œâ”€ case_name_001                 (Group)\n",
    "â”‚  â”‚     â”‚  â”œâ”€ wind_field                 (Dataset)\n",
    "â”‚  â”‚     â”‚  â”‚  â”œâ”€ shape: (C, H, W)        # channels-first (CHW)\n",
    "â”‚  â”‚     â”‚  â”‚  â”œâ”€ dtype: float16\n",
    "â”‚  â”‚     â”‚  â”‚  â””â”€ compression: gzip\n",
    "â”‚  â”‚     â”‚  â””â”€ attrs[\"meta\"]               (string, JSON)\n",
    "â”‚  â”‚     â”‚     â””â”€ example: '{\"cityHW\":[H,W], \"wind_dir_deg\": 45, \"wind_dir_xy\":[0.7,0.7], \"channels\": [\"u\",\"v\", ...]}'\n",
    "â”‚  â”‚     â”œâ”€ case_name_002                 (Group)\n",
    "â”‚  â”‚     â”‚  â””â”€ ...\n",
    "â”‚  â”‚     â””â”€ ...\n",
    "â”‚  â”œâ”€ CityB.h5\n",
    "â”‚  â”‚  â””â”€ ... (same layout as CityA.h5)\n",
    "â”‚  â””â”€ GlobalMeta.h5\n",
    "â”‚     â”œâ”€ attrs[\"num_cities\"]              (int)\n",
    "â”‚     â”œâ”€ city_list                        (Dataset, dtype='S')  # list of city names (bytes)\n",
    "â”‚     â””â”€ attrs[\"description\"]             (string)\n",
    "â”‚\n",
    "â”œâ”€ val/\n",
    "â”‚  â”œâ”€ CityX.h5\n",
    "â”‚  â”‚  â””â”€ ... (same layout as train/*.h5)\n",
    "â”‚  â””â”€ GlobalMeta.h5\n",
    "â”‚     â””â”€ ... (metadata for val set)\n",
    "â””â”€ (optional: other files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2057bbb",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf45a902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ cities = 32\n",
      "train cities: 25 | val cities: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AU-Syd-U1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.52it/s]\n",
      "AU-Syd-U2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 13.12it/s]\n",
      "AU-Syd-U3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 45.96it/s]\n",
      "AU-Syd-U4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.76it/s]\n",
      "AU-Syd-U5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.32it/s]\n",
      "AU-Syd-U6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.49it/s]\n",
      "AU-Syd-U7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.52it/s]\n",
      "AU-SYD-V1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.35it/s]\n",
      "AU-SYD-V2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.80it/s]\n",
      "CN-BE-V1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.41it/s]\n",
      "CN-BE-V2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.91it/s]\n",
      "JP-Tok-U2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.12it/s]\n",
      "JP-Tok-V1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.77it/s]\n",
      "JP-Tok-V3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.36it/s]\n",
      "JP-Tok-V4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.10it/s]\n",
      "KO-SE-U1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.98it/s]\n",
      "KO-SE-U2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.67it/s]\n",
      "KO-SE-U3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.30it/s]\n",
      "KO-SE-U4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.26it/s]\n",
      "KO-SE-V10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.75it/s]\n",
      "KO-SE-V11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.71it/s]\n",
      "KO-SE-V1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.17it/s]\n",
      "KO-SE-V2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.96it/s]\n",
      "KO-SE-V3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.13it/s]\n",
      "KO-SE-V4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.66it/s]\n",
      "KO-SE-V5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.55it/s]\n",
      "KO-SE-V6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.24it/s]\n",
      "KO-SE-V7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.10it/s]\n",
      "KO-SE-V8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.68it/s]\n",
      "KO-SE-V9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.51it/s]\n",
      "NL-DEL-U2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.31it/s]\n",
      "NL-DEL-V2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å®Œæˆ train/val åŸŽå¸‚åˆ‡åˆ†èˆ‡ H5 å»ºç«‹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== config ====\n",
    "input_folder = Path(\"../dataset\")\n",
    "root_out = Path(\"../dataset_h5\")\n",
    "train_dir = root_out / \"train\"\n",
    "val_dir = root_out / \"val\"\n",
    "train_ratio = 0.8\n",
    "\n",
    "# ==== clean output ====\n",
    "if root_out.exists():\n",
    "    for p in root_out.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            p.unlink()\n",
    "else:\n",
    "    root_out.mkdir()\n",
    "\n",
    "train_dir.mkdir(exist_ok=True, parents=True)\n",
    "val_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# === gather city cases ===\n",
    "city_cases = {}\n",
    "for ped_file in input_folder.rglob(\"*ped.nc\"):\n",
    "    case_name = ped_file.parent.name\n",
    "    city_name, _, _ = extract_city_name_and_wind(case_name)\n",
    "    city_cases.setdefault(city_name, []).append(ped_file)\n",
    "\n",
    "print(f\"ðŸ“¦ cities = {len(city_cases)}\")\n",
    "\n",
    "# === city-level split ===\n",
    "all_cities = list(city_cases.keys())\n",
    "random.shuffle(all_cities)\n",
    "\n",
    "train_count = int(len(all_cities) * train_ratio)\n",
    "train_cities = set(all_cities[:train_count])\n",
    "val_cities = set(all_cities[train_count:])\n",
    "\n",
    "print(f\"train cities: {len(train_cities)} | val cities: {len(val_cities)}\")\n",
    "\n",
    "def save_city_to_h5(city_name, ped_files, out_path):\n",
    "    with h5py.File(out_path, \"w\") as h5f:\n",
    "        # ç¬¬ä¸€å€‹case â†’ city_base\n",
    "        city_base_np, _, city_base_channels, _, _ = process_ped_file(ped_files[0])\n",
    "        h5f.create_dataset(\"city_base\", data=city_base_np.astype(np.float16), compression=\"gzip\")\n",
    "\n",
    "        cases_grp = h5f.create_group(\"cases\")\n",
    "\n",
    "        for ped_file in tqdm(ped_files, desc=f\"{city_name}\"):\n",
    "            case_name = ped_file.parent.name\n",
    "            _, case_data, _, case_ch, (H,W) = process_ped_file(ped_file)\n",
    "\n",
    "            # flip Y if needed\n",
    "            case_data = case_data[:, ::-1]\n",
    "\n",
    "            _, wind_deg, (wx, wy) = extract_city_name_and_wind(case_name)\n",
    "            meta = dict(\n",
    "                cityHW=[H,W],\n",
    "                wind_dir_deg=wind_deg,\n",
    "                wind_dir_xy=[wx, wy],\n",
    "                channels=case_ch\n",
    "            )\n",
    "\n",
    "            grp = cases_grp.create_group(case_name)\n",
    "            grp.create_dataset(\"wind_field\", data=case_data.astype(np.float16), compression=\"gzip\")\n",
    "            grp.attrs[\"meta\"] = json.dumps(meta)\n",
    "\n",
    "# === write h5 ===\n",
    "for city_name, ped_files in city_cases.items():\n",
    "    out_dir = train_dir if city_name in train_cities else val_dir\n",
    "    out_path = out_dir / f\"{city_name}.h5\"\n",
    "    save_city_to_h5(city_name, ped_files, out_path)\n",
    "\n",
    "# === save metadata ===\n",
    "def write_global_metadata(path, city_list):\n",
    "    with h5py.File(path, \"w\") as f:\n",
    "        f.attrs[\"num_cities\"] = len(city_list)\n",
    "        f.create_dataset(\"city_list\",\n",
    "            data=np.array([c.encode() for c in city_list], dtype=\"S\"),\n",
    "            compression=\"gzip\")\n",
    "        f.attrs[\"description\"] = \"GNCA wind-field dataset split by city\"\n",
    "\n",
    "write_global_metadata(train_dir / \"GlobalMeta.h5\", list(train_cities))\n",
    "write_global_metadata(val_dir / \"GlobalMeta.h5\", list(val_cities))\n",
    "\n",
    "print(\"âœ… å®Œæˆ train/val åŸŽå¸‚åˆ‡åˆ†èˆ‡ H5 å»ºç«‹\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
