{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b343a06",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ad1a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment initialized. Use show_env_info() to check details.\n",
      "Using device: cuda\n",
      "[INFO] Global seed set to 1234\n"
     ]
    }
   ],
   "source": [
    "# 初始化整個實驗環境\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/GAI/Desktop/Scott/NCA_Research\")\n",
    "\n",
    "from E4_PI_NCA.init_notebook_imports import *\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "set_global_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba43e3f",
   "metadata": {},
   "source": [
    "# data process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eedfee",
   "metadata": {},
   "source": [
    "## func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1767c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 輔助函式\n",
    "# ================================================================\n",
    "def nan_to_masked_CHW(arr):\n",
    "    \"\"\"將 NaN 轉成 0，並新增一個 geo_mask channel（1 表有效）\"\"\"\n",
    "    mask = (~np.isnan(arr[0])).astype(np.float32)[np.newaxis, ...]\n",
    "    arr = np.nan_to_num(arr, nan=0.0)\n",
    "    return np.concatenate([mask, arr], axis=0)\n",
    "\n",
    "def add_coord_channels(arr):\n",
    "    \"\"\"在最前面新增 normalized coord_y, coord_x\"\"\"\n",
    "    _, H, W = arr.shape\n",
    "    y = np.linspace(-1, 1, H)[..., None] * np.ones((1, W))\n",
    "    x = np.linspace(-1, 1, W)[None, ...] * np.ones((H, 1))\n",
    "    coords = np.stack([y, x], axis=0)\n",
    "    return np.concatenate([coords, arr], axis=0)\n",
    "\n",
    "def center_crop(arr, target_h, target_w):\n",
    "    \"\"\"以中心為基準裁切 (C, H, W)\"\"\"\n",
    "    C, H, W = arr.shape\n",
    "    start_h = (H - target_h) // 2\n",
    "    start_w = (W - target_w) // 2\n",
    "    return arr[:, start_h:start_h+target_h, start_w:start_w+target_w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a5f3f",
   "metadata": {},
   "source": [
    "# process urbantales cases into npz file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7475ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ped files: 100%|██████████| 72/72 [00:09<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Channel-wise stats (共 11 個 channel):\n",
      "ch           min         q1       mean         q3        max\n",
      "------------------------------------------------------------\n",
      "0      -1.000184  -0.499782   0.000000   0.499782   1.000184\n",
      "1      -1.001059  -0.500806  -0.000000   0.500806   1.001059\n",
      "2       1.000000   1.000000   1.000000   1.000000   1.000000\n",
      "3     -48.800720   0.000000   4.752388   2.923584 225.204889\n",
      "4       0.000000   0.000000   0.366546   0.707107   1.000000\n",
      "5      -1.000000   0.439705   0.642144   1.000000   1.000000\n",
      "6      -2.027470  -0.066149   0.075328   0.184772   1.807298\n",
      "7      -1.832599  -0.043820   0.059269   0.159883   1.698117\n",
      "8      -0.254789   0.057104   0.311276   0.485398   2.165388\n",
      "9      -0.062029   0.015168   0.053031   0.079356   0.728069\n",
      "10     -0.190695  -0.003430  -0.001935   0.000000   0.276433\n",
      "\n",
      "✅ 已儲存：..\\dataset\\all_cases_BCHW.npz\n",
      "共 72 個 case\n",
      "資料形狀：(72, 11, 256, 256) (B, C, H, W)\n",
      " 通道名稱 channel: ['coord_y', 'coord_x', 'geo_mask', 'topo', 'windInitX', 'windInitY', 'uped', 'vped', 'Uped', 'TKEped', 'Tuwped']\n",
      "範例 case: AU-Syd-U1_d00\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ================================================================\n",
    "# 基本設定\n",
    "# ================================================================\n",
    "folder = Path(\"../dataset\")\n",
    "output_path = folder / \"all_cases_BCHW.npz\"\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 主流程：讀取與處理所有 case\n",
    "# ================================================================\n",
    "all_cases = []\n",
    "case_names = []\n",
    "channel_names_ref = None\n",
    "from tqdm import tqdm\n",
    "count=0\n",
    "for ped_file in tqdm(list(folder.rglob(\"*ped.nc\")), desc=\"Processing ped files\"):\n",
    "    case_name = ped_file.parent.name\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 解析風向 (從資料夾名稱中的 _d 取得角度)\n",
    "    # ------------------------------------------------------------\n",
    "    wind_dir = float(case_name.split(\"_d\")[-1])\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 讀取 NetCDF\n",
    "    # ------------------------------------------------------------\n",
    "    with xr.open_dataset(ped_file) as ds:\n",
    "        arrays = [ds[var].values for var in ds.data_vars]\n",
    "        ped_np = np.stack(arrays, axis=-1)[::-1, :, :]  # (H, W, C)\n",
    "        vars_names = list(ds.data_vars.keys())\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 加入 wind direction (sin, cos)\n",
    "    # ------------------------------------------------------------\n",
    "    wind_np = np.zeros_like(ped_np[..., 0:2])\n",
    "    wind_np[..., 0] = np.sin(np.deg2rad(wind_dir))\n",
    "    wind_np[..., 1] = np.cos(np.deg2rad(wind_dir))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 加入 topo 資料（若存在）\n",
    "    # ------------------------------------------------------------\n",
    "    topo_file = next(ped_file.parent.glob(\"*_topo\"), None)\n",
    "    if topo_file:\n",
    "        topo = np.loadtxt(topo_file)[:, :, np.newaxis]\n",
    "        ped_np = np.concatenate([topo, wind_np, ped_np], axis=-1)\n",
    "        channel_names = [\"topo\", \"windInitX\", \"windInitY\"] + vars_names\n",
    "    else:\n",
    "        ped_np = np.concatenate([wind_np, ped_np], axis=-1)\n",
    "        channel_names = [\"windInitX\", \"windInitY\"] + vars_names\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 後處理\n",
    "    # ------------------------------------------------------------\n",
    "    # HWC → CHW\n",
    "    ped_np = np.transpose(ped_np, (2, 0, 1))\n",
    "    # NaN 處理與 mask\n",
    "    ped_np = nan_to_masked_CHW(ped_np)\n",
    "    # 加上 coord channel\n",
    "    ped_np = add_coord_channels(ped_np)\n",
    "    # 更新 channel 名稱\n",
    "    channel_names = [\"coord_y\", \"coord_x\", \"geo_mask\"] + channel_names\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Resize / Crop\n",
    "    # ------------------------------------------------------------\n",
    "    ped_t = torch.from_numpy(ped_np).unsqueeze(0)  # (1, C, H, W)\n",
    "    _, C, H, W = ped_t.shape\n",
    "    # scale = 3\n",
    "    # final_size = 96\n",
    "    # ped_i = F.interpolate(ped_t, size=(H//scale, W//scale), mode=\"nearest\")\n",
    "    # ped_np = ped_i.squeeze(0).numpy()\n",
    "    # ped_np = center_crop(ped_np, final_size,final_size)  # 保留最終尺寸一致\n",
    "    # print(f\"處理 {case_name} ... shape = {ped_t.shape}  | shape = {ped_i.shape}  | shape = {ped_np.shape}\")\n",
    "\n",
    "\n",
    "    ped_i = F.interpolate(ped_t, size=(256,256), mode=\"bicubic\")\n",
    "    ped_np = ped_i.squeeze(0).numpy()\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 累積\n",
    "    # ------------------------------------------------------------\n",
    "    all_cases.append(ped_np)\n",
    "    case_names.append(case_name)\n",
    "    if channel_names_ref is None:\n",
    "        channel_names_ref = channel_names\n",
    "    \n",
    "    # count+=1\n",
    "    # if count>4:\n",
    "    #     break\n",
    "\n",
    "# ================================================================\n",
    "# 統一合併為 BCHW\n",
    "# ================================================================\n",
    "data_BCHW = np.stack(all_cases, axis=0)  # (B, C, H, W)\n",
    "print_tensor_stats(data_BCHW)\n",
    "# ================================================================\n",
    "# 存檔\n",
    "# ================================================================\n",
    "np.savez_compressed(\n",
    "    output_path,\n",
    "    data=data_BCHW,\n",
    "    case_names=np.array(case_names),\n",
    "    channel_names=np.array(channel_names_ref)\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# 驗證輸出\n",
    "# ================================================================\n",
    "print(f\"\\n✅ 已儲存：{output_path}\")\n",
    "print(f\"共 {len(case_names)} 個 case\")\n",
    "print(f\"資料形狀：{data_BCHW.shape} (B, C, H, W)\")\n",
    "print(\" 通道名稱 channel:\", channel_names_ref)\n",
    "print(\"範例 case:\", case_names[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb072051",
   "metadata": {},
   "source": [
    "'uped', 'vped' vel_ped 平均風速(不包含湍流) Uped 平均風速(包含湍流)  TKEped(湍流項)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c079848",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc0ed7",
   "metadata": {},
   "source": [
    "## func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5320d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def plot_BCHW_channels(\n",
    "    data_BCHW,\n",
    "    case_names,\n",
    "    channel_names,\n",
    "    max_channels: int = 9,\n",
    "    filter_patterns: list = []\n",
    "):\n",
    "    \"\"\"\n",
    "    將 BCHW 資料繪製成每個 case 一行、每個 channel 一列的圖。\n",
    "    支援正規表達式過濾樣本名稱。\n",
    "\n",
    "    Args:\n",
    "        data_BCHW : np.ndarray | torch.Tensor\n",
    "            shape = (B, C, H, W)\n",
    "        case_names : list[str] or np.ndarray\n",
    "            對應每個樣本的名稱\n",
    "        channel_names : list[str] or np.ndarray\n",
    "            每個 channel 的名稱\n",
    "        max_channels : int, optional\n",
    "            每個樣本最多顯示多少 channel (default: 9)\n",
    "        filter_patterns : list[str], optional\n",
    "            要排除的樣本名稱 regex pattern\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 型態與資料檢查\n",
    "    # ------------------------------------------------------------\n",
    "    if data_BCHW.ndim != 4:\n",
    "        raise ValueError(f\"輸入資料需為 4 維 (B,C,H,W)，但得到 ndim={data_BCHW.ndim}\")\n",
    "\n",
    "    if isinstance(data_BCHW, torch.Tensor):\n",
    "        data_BCHW = data_BCHW.detach().cpu().numpy()\n",
    "\n",
    "    B, C, H, W = data_BCHW.shape\n",
    "\n",
    "    case_names = np.array(case_names)\n",
    "    channel_names = np.array(channel_names)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 過濾樣本 (使用 regex)\n",
    "    # ------------------------------------------------------------\n",
    "    valid_indices = [\n",
    "        i for i, name in enumerate(case_names)\n",
    "        if not any(re.match(pat, name) for pat in filter_patterns)\n",
    "    ]\n",
    "    data_BCHW = data_BCHW[valid_indices]\n",
    "    case_names = case_names[valid_indices]\n",
    "\n",
    "    n_samples = len(case_names)\n",
    "    n_show = min(max_channels, C)\n",
    "\n",
    "    print(f\"繪製 {n_samples} 個樣本，每個顯示前 {n_show} 個 channel\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 建立 subplot\n",
    "    # ------------------------------------------------------------\n",
    "    ncols = n_show + 2  # case name + 原圖 + channels\n",
    "    fig, axes = plt.subplots(nrows=n_samples, ncols=ncols, figsize=(3*ncols, 3*n_samples))\n",
    "\n",
    "    if n_samples == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "    if ncols == 1:\n",
    "        axes = np.expand_dims(axes, 1)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 主繪圖迴圈\n",
    "    # ------------------------------------------------------------\n",
    "    for i, (case_name, chw) in enumerate(zip(case_names, data_BCHW)):\n",
    "        HWC = np.transpose(chw, (1, 2, 0))\n",
    "\n",
    "        # 第一欄：case 名稱\n",
    "        axes[i][0].text(0.5, 0.5, str(case_name), ha=\"center\", va=\"center\",\n",
    "                        fontsize=12, weight=\"bold\")\n",
    "        axes[i][0].axis(\"off\")\n",
    "\n",
    "        # 第二欄：前三 channel 合成原圖\n",
    "        if C >= 3:\n",
    "            axes[i][1].imshow(HWC[:, :, :3])\n",
    "        else:\n",
    "            axes[i][1].imshow(HWC[:, :, 0], cmap=\"gray\")\n",
    "        axes[i][1].set_title(\"原圖\")\n",
    "        axes[i][1].axis(\"off\")\n",
    "\n",
    "        # 其餘欄：各 channel\n",
    "        for j in range(n_show):\n",
    "            ch_data = chw[j]\n",
    "            ax = axes[i][j + 2]\n",
    "            im = ax.imshow(ch_data, cmap=\"jet\")\n",
    "            title = channel_names[j] if j < len(channel_names) else f\"ch{j}\"\n",
    "            ax.set_title(title)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "            # 若通道為常數 → 顯示數值\n",
    "            if np.all(ch_data == ch_data.flat[0]):\n",
    "                ax.text(W // 2, H // 2, f\"{ch_data.flat[0]:.3f}\",\n",
    "                        color=\"white\", ha=\"center\", va=\"center\",\n",
    "                        fontsize=10, weight=\"bold\",\n",
    "                        bbox=dict(facecolor=\"black\", alpha=0.6, boxstyle=\"round,pad=0.2\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"channels.png\",dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425601e8",
   "metadata": {},
   "source": [
    "## viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a154e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Channel-wise stats (共 11 個 channel):\n",
      "ch           min         q1       mean         q3        max\n",
      "------------------------------------------------------------\n",
      "0      -1.000184  -0.499782   0.000000   0.499782   1.000184\n",
      "1      -1.001059  -0.500806  -0.000000   0.500806   1.001059\n",
      "2       1.000000   1.000000   1.000000   1.000000   1.000000\n",
      "3     -48.800720   0.000000   4.752388   2.923584 225.204889\n",
      "4       0.000000   0.000000   0.366546   0.707107   1.000000\n",
      "5      -1.000000   0.439705   0.642144   1.000000   1.000000\n",
      "6      -2.027470  -0.066149   0.075328   0.184772   1.807298\n",
      "7      -1.832599  -0.043820   0.059269   0.159883   1.698117\n",
      "8      -0.254789   0.057104   0.311276   0.485398   2.165388\n",
      "9      -0.062029   0.015168   0.053031   0.079356   0.728069\n",
      "10     -0.190695  -0.003430  -0.001935   0.000000   0.276433\n",
      "繪製 58 個樣本，每個顯示前 11 個 channel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9994703642425048..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9991396157522389..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9991396157522389..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9991396157522389..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9991396157522389..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9991396157522389..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0010587155462025..1.0010587155462025].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0010587155462025..1.0010587155462025].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.999672919615636..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.999672919615636..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9981572690217393..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9987565326026864..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9987565326026864..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9992946795563201..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342238..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9975917641078648..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.997090076902957..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9979714627824666..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.997090076902957..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9977839963091468..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9979714627824665..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9987565326026863..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282101..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.998345651934224..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9990004895561357..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342236..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342236..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342236..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342236..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342236..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9979714627824665..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9979714627824665..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9979714627824665..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9979714627824665..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9987565326026863..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282101..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282101..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282101..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282101..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282101..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.998345651934224..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.998345651934224..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.998345651934224..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.998345651934224..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.998345651934224..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9981572690217393..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9994703642425048..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9990004895561357..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282099..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282099..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282099..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282099..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9985425467282099..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9987565326026863..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342238..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342238..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342238..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9983456519342238..1.0].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m channel_names = npz[\u001b[33m\"\u001b[39m\u001b[33mchannel_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      7\u001b[39m print_tensor_stats(data_BCHW)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mplot_BCHW_channels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_BCHW\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_BCHW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcase_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchannel_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannel_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m11\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m^CN-\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== 0-1 scale 後 ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m data_BCHW = minmax_scale_channelwise(data_BCHW)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mplot_BCHW_channels\u001b[39m\u001b[34m(data_BCHW, case_names, channel_names, max_channels, filter_patterns)\u001b[39m\n\u001b[32m    103\u001b[39m plt.tight_layout()\n\u001b[32m    104\u001b[39m plt.savefig(\u001b[33m\"\u001b[39m\u001b[33mchannels.png\u001b[39m\u001b[33m\"\u001b[39m,dpi=\u001b[32m300\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\pyplot.py:614\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    570\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03mDisplay all open figures.\u001b[39;00m\n\u001b[32m    572\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    611\u001b[39m \u001b[33;03mexplicitly there.\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    613\u001b[39m _warn_if_gui_out_of_main_thread()\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(close, block)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf.get_all_fig_managers():\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     95\u001b[39m     show._to_draw = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\IPython\\core\\display_functions.py:278\u001b[39m, in \u001b[36mdisplay\u001b[39m\u001b[34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m     publish_display_data(data=obj, metadata=metadata, **kwargs)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     format_dict, md_dict = \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[32m    280\u001b[39m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\IPython\\core\\formatters.py:238\u001b[39m, in \u001b[36mDisplayFormatter.format\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m    236\u001b[39m md = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     data = \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\decorator.py:232\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    231\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\IPython\\core\\formatters.py:282\u001b[39m, in \u001b[36mcatch_format_error\u001b[39m\u001b[34m(method, self, *args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     r = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\IPython\\core\\formatters.py:402\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[32m    404\u001b[39m method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170\u001b[39m, in \u001b[36mprint_figure\u001b[39m\u001b[34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[32m    168\u001b[39m     FigureCanvasBase(fig)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m data = bytes_io.getvalue()\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fmt == \u001b[33m'\u001b[39m\u001b[33msvg\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\backend_bases.py:2160\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[32m   2159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches == \u001b[33m\"\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2160\u001b[39m         bbox_inches = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2162\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(layout_engine, ConstrainedLayoutEngine) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   2163\u001b[39m                 pad_inches == \u001b[33m\"\u001b[39m\u001b[33mlayout\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2164\u001b[39m             h_pad = layout_engine.get()[\u001b[33m\"\u001b[39m\u001b[33mh_pad\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\figure.py:1848\u001b[39m, in \u001b[36mFigureBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, bbox_extra_artists)\u001b[39m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax.get_visible():\n\u001b[32m   1845\u001b[39m     \u001b[38;5;66;03m# some Axes don't take the bbox_extra_artists kwarg so we\u001b[39;00m\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# need this conditional....\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m         bbox = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1851\u001b[39m         bbox = ax.get_tightbbox(renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:4587\u001b[39m, in \u001b[36m_AxesBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[39m\n\u001b[32m   4584\u001b[39m     bbox_artists = \u001b[38;5;28mself\u001b[39m.get_default_bbox_extra_artists()\n\u001b[32m   4586\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m bbox_artists:\n\u001b[32m-> \u001b[39m\u001b[32m4587\u001b[39m     bbox = \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4588\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4589\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m < bbox.width < np.inf\n\u001b[32m   4590\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m < bbox.height < np.inf):\n\u001b[32m   4591\u001b[39m         bb.append(bbox)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\artist.py:364\u001b[39m, in \u001b[36mArtist.get_tightbbox\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_tightbbox\u001b[39m(\u001b[38;5;28mself\u001b[39m, renderer=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    349\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[33;03m    Like `.Artist.get_window_extent`, but includes any clipping.\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    362\u001b[39m \u001b[33;03m        Returns None if clipping results in no intersection.\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     bbox = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_clip_on():\n\u001b[32m    366\u001b[39m         clip_box = \u001b[38;5;28mself\u001b[39m.get_clip_box()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\patches.py:655\u001b[39m, in \u001b[36mPatch.get_window_extent\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_window_extent\u001b[39m(\u001b[38;5;28mself\u001b[39m, renderer=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_extents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\path.py:645\u001b[39m, in \u001b[36mPath.get_extents\u001b[39m\u001b[34m(self, transform, **kwargs)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bbox\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m     \u001b[38;5;28mself\u001b[39m = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.codes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    647\u001b[39m     xys = \u001b[38;5;28mself\u001b[39m.vertices\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\transforms.py:1601\u001b[39m, in \u001b[36mTransform.transform_path\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1594\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform_path\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[32m   1595\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1596\u001b[39m \u001b[33;03m    Apply the transform to `.Path` *path*, returning a new `.Path`.\u001b[39;00m\n\u001b[32m   1597\u001b[39m \n\u001b[32m   1598\u001b[39m \u001b[33;03m    In some cases, this transform may insert curves into the path\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    that began as line segments.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_path_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_path_non_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\transforms.py:1611\u001b[39m, in \u001b[36mTransform.transform_path_affine\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform_path_affine\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1605\u001b[39m \u001b[33;03m    Apply the affine part of this transform to `.Path` *path*, returning a\u001b[39;00m\n\u001b[32m   1606\u001b[39m \u001b[33;03m    new `.Path`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1609\u001b[39m \u001b[33;03m    ``transform_path_affine(transform_path_non_affine(values))``.\u001b[39;00m\n\u001b[32m   1610\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.transform_path_affine(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GAI\\miniconda3\\envs\\pytorch-py311\\Lib\\site-packages\\matplotlib\\transforms.py:2437\u001b[39m, in \u001b[36mCompositeGenericTransform.get_affine\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._b.get_affine()\n\u001b[32m   2436\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2437\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Affine2D(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_b\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_a\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 範例呼叫\n",
    "npz_path=\"../dataset/all_cases_BCHW.npz\"\n",
    "npz = np.load(npz_path, allow_pickle=True)\n",
    "data_BCHW = npz[\"data\"]\n",
    "case_names = npz[\"case_names\"]\n",
    "channel_names = npz[\"channel_names\"]\n",
    "print_tensor_stats(data_BCHW)\n",
    "plot_BCHW_channels(\n",
    "    data_BCHW=data_BCHW,\n",
    "    case_names=case_names,\n",
    "    channel_names=channel_names,\n",
    "    max_channels=11,\n",
    "    filter_patterns=[\"^CN-\"]\n",
    ")\n",
    "print(\"=== 0-1 scale 後 ===\")\n",
    "\n",
    "data_BCHW = minmax_scale_channelwise(data_BCHW)\n",
    "print_tensor_stats(data_BCHW)\n",
    "\n",
    "# 呼叫繪圖函式\n",
    "plot_BCHW_channels(\n",
    "    data_BCHW=data_BCHW,\n",
    "    case_names=case_names,\n",
    "    channel_names=channel_names,\n",
    "    max_channels=11,\n",
    "    filter_patterns=[\"^CN-\"]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
