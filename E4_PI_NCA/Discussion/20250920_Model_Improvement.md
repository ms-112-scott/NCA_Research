# 模型改進討論紀錄 (2025-09-20)

本文件針對 `E4-4.2_UrbanTales_GrowthNCA_v2.ipynb` 中遇到的問題，以及後續研究方向進行討論與建議。

---

### 問題一：模型難以訓練，無法有效 Overfit

> **使用者問題**: 模型目前的泛化能力太差，甚至連 Overfit 訓練集都有點困難。

這是一個典型的**模型表達能力不足 (Underfitting)** 或訓練策略不當的信號。當模型無法有效擬合訓練資料時，談論泛化能力還為時過早。首要目標應該是讓模型有足夠的能力「記住」訓練資料。

**建議方向**:

1.  **提升模型容量 (Model Capacity)**:
    *   **加深/加寬更新網路**: 目前 NCA 的更新規則可能過於簡單，無法捕捉流體動力學的複雜性。可以嘗試增加更新網路 (通常是一個小型 CNN) 的層數或每一層的通道 (channels) 數量。
    *   **豐富感知維度**: 固定的 Sobel/Laplacian 卷積核可能無法提供足夠的環境資訊。可以考慮增加更多的感知通道，例如：
        *   不同大小的卷積核 (e.g., 5x5, 7x7) 來感知更大範圍的環境。
        *   高斯模糊核來感知平均場的資訊。

2.  **優化訓練策略**:
    *   **從單一樣本開始**: 先嘗試讓模型 Overfit **單一一個** 訓練樣本 (一張圖片或一個短序列)。如果連單一樣本都無法完美重現，那幾乎可以肯定是模型結構或程式碼本身存在問題。
    *   **學習率調整 (Learning Rate)**: 檢查目前的學習率是否過高（導致梯度爆炸、不收斂）或過低（收斂過慢）。可以引入學習率排程器 (LR Scheduler)，如 `ReduceLROnPlateau`，當 loss 不再下降時自動降低學習率。
    *   **增加訓練迭代**: 確認模型是否進行了足夠的訓練迭代次數 (epochs/steps)。

3.  **檢查損失函數 (Loss Function)**:
    *   除了 MSE Loss，可以考慮引入 **感知損失 (Perceptual Loss, e.g., LPIPS)**。MSE 傾向於產生模糊的平均結果，而感知損失更關注結構相似性，有時能引導模型學習到更合理的紋理與結構，從而更容易收斂。

---

### 問題二：輸入尺寸過大，Crop 導致收斂困難

> **使用者問題**: 資料集的 2D shape 太大，我試圖 crop，但會導致模型不易收斂。我不想用 resize，因為會破壞 CFD 影像的空間尺度物理特性。

您的堅持非常正確！Resize 會扭曲物理網格，是不可取的。Crop 導致收斂困難，通常是因為邊界資訊丟失，使得 Crop 出來的 Patch 物理上「不完整」。

**建議方向**:

1.  **Patch-Based 訓練 (基於補丁的訓練)**:
    *   **核心思想**: NCA 的更新規則是局部的 (local)，一個細胞的狀態只受其鄰域影響。因此，只要訓練的 Patch 尺寸遠大於 NCA 的感知範圍，Patch 中心的細胞其行為就應該是物理正確的。
    *   **具體做法**: 將大的 CFD 網格切分成大量**重疊 (Overlapping) 的小 Patches**。例如，一個 512x512 的網格可以切分成數百個 64x64 的 Patches。重疊可以確保每個細胞都有機會被當作「中心細胞」來訓練。
    *   **邊緣損失加權**: 在計算單一 Patch 的 Loss 時，可以對邊緣像素的 Loss 賦予較低的權重，因為它們的感知是不完整的。讓 Loss 的主要貢獻來自於 Patch 的中心區域。

2.  **全卷積網路 (Fully Convolutional Network, FCN) 思想**:
    *   NCA 本身就是一種 FCN。理論上，它可以在任意尺寸的輸入上進行訓練和推論。訓練時使用 Patches 以節省記憶體，但在推論 (Inference) 時，可以直接將訓練好的模型應用在完整的、未經 Crop 的大尺寸網格上。

---

### 問題三：PINN 物理約束的引入時機

> **使用者問題**: 我目前還沒加入 PINN 約束，思路是先讓模型能 Overfit 資料本身，再加入 PINN 控制泛化性。

這個思路**非常正確**，是訓練複雜模型（尤其是多任務、多損失的模型）時的最佳實踐。這本質上是一種**課程學習 (Curriculum Learning)**。

**建議與細化**: 

1.  **階段一：純資料驅動**: 完全專注於讓模型擬合資料。此階段的目標是驗證模型有足夠的**表達能力**。

2.  **階段二：物理微調 (Physics Fine-tuning)**: 當模型在資料上達到一個不錯的表現後，凍結部分權重或使用一個更小的學習率，並**引入物理損失項**。
    *   **損失權重緩增**: 引入物理損失時，其權重 (`lambda`) 應該從一個非常小的值開始 (e.g., `loss = data_loss + 0.001 * physics_loss`)，然後在訓練過程中逐漸增加。這可以防止新的、可能很大的物理損失梯度「衝擊」已經學好的模型權重，導致訓練不穩定。
    *   **分開監控 Loss**: 在訓練日誌中，務必將 `data_loss` 和 `physics_loss` 分開記錄。理想的狀況是，`physics_loss` 穩定下降，同時 `data_loss` 沒有顯著惡化。

---

### 問題四：長時間演化 Loss 爆炸 & 未來方向 (LBM-NCA)

> **使用者問題**: 應用時長時間演化的 Loss 容易累加爆炸。後續希望把 LBM 當成 PINN 的來源與限制，加入到 NCA 中。

這兩個問題是相關的：Loss 爆炸反映了模型學到的「規則」在時間上不穩定，而 LBM 正是一種以其穩定性和局部性著稱的、與 CA 高度契合的物理模型。

**A. 解決長時間演化不穩定的問題**

1.  **多步損失 (Multi-step Loss)**: 在訓練時，不要只計算下一步 (`t+1`) 的 Loss，而是讓模型一次性演化 N 步，然後計算這 N 步累積的 Loss (`loss(t+1) + loss(t+2) + ... + loss(t+N)`)。這會迫使模型學習到在時間上更穩定的演化規則。

2.  **計畫性抽樣 (Scheduled Sampling)**: 在訓練的早期，下一步的輸入總是使用真實資料 (Ground Truth)。隨著訓練的進行，逐漸增加使用**模型自己的預測**作為下一步輸入的概率。這等於是提前讓模型「適應」自己可能會犯的錯誤，從而學會如何從自己的小錯誤中恢復，而不是讓錯誤無限放大。

**B. 未來的研究方向：LBM 作為 NCA 的物理核心**

這個想法**非常棒，而且極具洞察力**。相比於在離散的網格上用有限差分去近似連續的 Navier-Stokes 方程式，LBM 本身就是一個定義在離散網格和離散速度方向上的模型，其「串流-碰撞」的更新模式與 NCA 的「感知-更新」模式有著驚人的相似性，是一個更「原生」的結合方式。

**實現思路**: 

1.  **NCA 狀態的重新定義**: 將 NCA 的隱狀態 (hidden state) 定義為 LBM 中的**粒子分佈函數 (distribution functions)**，例如 D2Q9 模型中的 9 個方向的分佈。

2.  **串流步 (Streaming Step)**: LBM 的串流步是一個固定的、確定性的操作（將粒子分佈傳遞給相鄰格子）。這一步可以實現為一個**固定的、不可訓練的卷積層**，其權重被設計為只進行資料的平移。

3.  **碰撞步 (Collision Step)**: LBM 的碰撞步是核心，它在每個格子內部獨立進行，使粒子分佈趨向局部平衡態。這一步正是神經網路可以替代和增強的地方。你可以**用一個小型的神經網路（例如一個 MLP）來實現碰撞算子**。這個網路的輸入是當前格子的 9 個分佈函數，輸出是碰撞後的 9 個分佈函數。

4.  **LBM-PINN 損失**: 物理損失不再是 Navier-Stokes 殘差，而是**LBM 的平衡態約束**。你可以計算由 NCA 狀態導出的宏觀量（密度、速度），然後計算出對應的平衡態分佈，損失函數就是讓 NCA 的狀態分佈去逼近這個理論上的平衡態分佈。

**優勢**: 這種 LBM-NCA 混合模型，既利用了 LBM 清晰的物理圖像和穩定性，又賦予了模型通過神經網路學習複雜、非線性碰撞規則的能力，是一個非常前沿且有潛力的研究方向。
