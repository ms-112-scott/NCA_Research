# 簡化版 CFD Iterative PINN (Urban Scale, Pedestrian-level)

## 1. 流程設計

1. **讀取輸入**

   - 幾何與網格資訊（建築 footprint, domain mask）
   - 邊界條件（Inlet 剖面、Outlet、Wall、Top/Lateral）
   - 步行者高度取樣物理量：

     - `uped` (streamwise velocity)
     - `vped` (spanwise velocity)
     - `Uped` (wind speed magnitude)
     - `TKEped` (turbulent kinetic energy)
     - `Tuwped` (turbulent momentum flux)

2. **邊界條件檢查 (BC check)**

   - 偵測輸入邊界是否有缺失或異常

3. **分支行為**

   - **若需要修補 → 執行 [BC repair] → 回到下一次迭代**
   - **若不需要修補 → 執行 [CFD PINN solver] → 更新 ΔU → 進入下一次迭代**

---

## 2. Input/Output 定義

### Input

- 幾何：Domain mask / 建築 footprint
- 座標點：(x,y)，高度固定 1.75m
- 邊界條件：

  - Inlet 剖面風速
  - Outlet 壓力或速度條件
  - Wall no-slip
  - Top / Lateral 條件

- 步行者高度物理量：

  - `uped` : streamwise velocity $(m/s)$
  - `vped` : spanwise velocity $(m/s)$
  - `Uped` : wind speed magnitude $(m/s)$
  - `TKEped` : turbulent kinetic energy $(m^2/s^2)$
  - `Tuwped` : turbulent momentum flux $(m^2/s^2)$

### Output

- **若執行 BC repair**：

  - 修正後的邊界條件 (不更新解場)

- **若執行 CFD solver**：

  - (\hat{u}(x,y)), (\hat{v}(x,y)) : 預測速度分量
  - (\hat{p}(x,y)) : 預測壓力場
  - ΔU : 更新解場用的速度修正量
  - （可選）(\hat{k}(x,y), \hat{T\_{uw}}(x,y))

---

## 3. Loss Function 構成

1. **Navier–Stokes 殘差 (Physics loss)**

   - 動量方程：
     [
     R_{mom} = \rho (\mathbf{U}\cdot\nabla \mathbf{U}) + \nabla p - \mu \nabla^2 \mathbf{U}
     ]
   - 不可壓縮條件：
     [
     R_{cont} = \nabla \cdot \mathbf{U} = 0
     ]

2. **邊界條件 Loss**

   - Inlet 剖面、Wall、Outlet

3. **數據對齊 Loss (可選)**

   - 與 LES 平均場 (`uped, vped, Uped, TKEped, Tuwped`) 的差異

---

## 4. 流程圖（文字版）

```
[讀取輸入資料]
        ↓
[BC 檢查] ──(需要修補?)──→ [BC repair]
        │                          │
        └─────(不需要)────→ [CFD PINN solver]
                                   ↓
                           [更新解場 ΔU]
                                   ↓
                           [進入下一次迭代]
```

---

# CFD Iterative PINN — Solver 設計（Urban, pedestrian-level, single slice）

**前提與風險提示**

- 資料為 LES time-averaged 在 z=1.75 m 的單切片（steady mean）
- 單層缺失垂直導數 (\partial*z) 等項，會造成不可辨識性；設計中用 (TKE*{ped}, Tuw\_{ped}) 作為額外監督以減少不確定性，但仍須謹慎驗證。（見 [[vertical_gap]]）

---

## 1. 目標（明確）

- 每次迭代輸出速度場修正量 (\Delta \mathbf{U}(x,y))（或直接輸出新場），使解場在多次迭代後趨近真實 mean field（最終滿足物理約束與監督資料）
- 中間步驟允許偏差，但**最終希望物理殘差與資料誤差同時小**
- 不做 sub-iteration：每次迭代只做一次 BC 檢查/可能修補，否則執行一次 solver → 更新 → 下次迭代

---

## 2. I/O（嚴格定義）

### 輸入（每次迭代）

- 座標：((x,y))（slice）
- 當前場（state）(\mathbf{U}^{(t)}(x,y) = [u^{(t)},v^{(t)}])（或只輸入場片段/patch）
- 邊界條件（已修復或原始）BC
- LES 提供的觀測 at slice：(uped, vped, Uped, TKEped, Tuwped)（在同一或部分點上）
- 幾何 mask（建築遮罩）與其他靜態條件（roughness 等）

### 輸出

- (\Delta \mathbf{U}^{(t)}(x,y) = [\Delta u, \Delta v])（推薦）

  - 更新規則： (\mathbf{U}^{(t+1)} = \mathbf{U}^{(t)} + \alpha \Delta \mathbf{U}^{(t)})（(\alpha)：relaxation）

- （可選）(\hat p(x,y))、(\hat k(x,y))、(\widehat{T\_{uw}}(x,y)) 作為輔助變數/監督輸出

---

## 3. 損失函數（分項、可加權）

總損失：

$$
\mathcal{L} = \lambda_{\text{data}}\mathcal{L}*{\text{data}} + \lambda*{\text{PDE}}\mathcal{L}*{\text{PDE}} + \lambda*{\text{BC}}\mathcal{L}*{\text{BC}} + \lambda*{\text{aux}}\mathcal{L}*{\text{aux}} + \lambda*{\text{reg}}\mathcal{L}_{\text{reg}}
$$

### (A) Data supervisory loss（在有 LES 觀測點）

$$
\mathcal{L}*{\text{data}} = \frac{1}{N_d}\sum*{i\in D}\Big( | \mathbf{U}^{\text{LES}}(x_i)-\mathbf{U}^{(t+1)}(x_i)|^2 + \gamma_U \big(U^{LES}-U^{(t+1)}\big)^2 \Big)
$$

- ( \mathbf{U}^{(t+1)}) 用更新後的場或直接用網路預測的場比較。
- 若 only (\Delta) 輸出，可在 loss 中使用 (\mathbf{U}^{(t)}+\alpha\Delta) 作為估計。

### (B) Physics loss（PDE residual，弱 enforcement）

在 collocation points (C)（domain interior）上：

$$
\mathcal{L}*{\text{PDE}} = \frac{1}{|C|}\sum*{x\in C}\Big( |R_{\text{mom}}(x)|^2 + \beta,|R_{\text{cont}}(x)|^2\Big)
$$

其中（incompressible, mean-field with effective viscosity (\nu\*\text{eff}=\nu+\nu_t)）

$$
R*{\text{mom}} = \rho (\mathbf{U}\cdot\nabla\mathbf{U}) + \nabla p - \nabla\cdot(\nu*\text{eff}\nabla\mathbf{U}) - \mathbf{G}(x)
$$

$$
R*{\text{cont}} = \nabla\cdot\mathbf{U}
$$

- (\mathbf{G}(x)) 代表未解析的垂直導數/雷諾項，可選擇讓網路同時輸出 (\mathbf{G}) 或用 closure（learn (\nu_t)）。
- 若輸出壓力，則用 (\hat p) 計算殘差；否則用投影或壓力消除法。

### (C) Boundary loss

在 BC points (B)：

$$
\mathcal{L}*{\text{BC}} = \frac{1}{|B|}\sum*{x\in B} |\mathcal{B}(\mathbf{U},p)-\text{BC}_{\text{target}}(x)|^2
$$

- 包括 inlet profile, wall no-slip（若 wall 在 slice 內）、outlet p 或速度條件

### (D) Auxiliary supervision（使用 TKE / Tuw）

$$
\mathcal{L}*{\text{aux}} = w_k \cdot \frac{1}{N_d}\sum*{i\in D} (\hat k(x_i)-k^{LES}(x_i))^2 + w_{tuw}\cdot \sum (\widehat{T_{uw}}-T_{uw}^{LES})^2
$$

- 如果網路輸出 (\hat k,\widehat{T\_{uw}})，用於抑制 closure 的自由度並提高可解釋性。

### (E) Regularization / Stability

- 權重衰減：(\lambda\_{\text{reg}} |\theta|^2)
- 平滑正則：(\int |\nabla \Delta \mathbf{U}|^2)（抑制高頻）
- 限幅懲罰（避免一次更新過大）： (\mathcal{L}_{\text{clip}} = \frac{1}{|C|}\sum \mathrm{ReLU}(|\alpha\Delta U| - \Delta_{\max})^2)

---

## 4. 如何處理壓力與 divergence-free（實務選項，需抉擇）

- **選項 1：顯式輸出壓力** (\hat p) 作為網路輔助輸出，直接在 (R\_{mom}) 用到。

  - 優點：直觀，殘差可直接評估
  - 缺點：壓力場未被觀測，可能造成非唯一性

- **選項 2：使用流函數/旋度（2D）**：參數化 (\mathbf{U}=\nabla\times\psi) 保證不可壓縮（僅 2D 適用）。

  - 若 2D 片層可行，這是強保守法。

- **選項 3：投影法**（在每次更新後做 divergence projection）

  - 先預測 (\mathbf{U}^\*), 再 solve Poisson for pressure correction（數值投影），等同在網路外面做物理解耦。
  - 混合物理+NN 的常用做法。

我建議在 urban slice（2D）可優先試 **流函數表述**或**顯式壓力輸出但加強正則與監督（aux loss）**，以便平衡可解性與自由度。

---

## 5. 訓練策略（實作步驟）

1. **資料前處理**

   - Non-dimensionalize（以 (U*{ref}), (L*{ref})）
   - Normalize inputs/outputs (zero mean, unit var)
   - 建立 mask、訓練點集 (D)、collocation (C)、boundary (B)

2. **Training stages**（建議）

   - Stage 0 — **預訓練 data-only**（only (\mathcal{L}_{\text{data}}+\mathcal{L}_{\text{aux}})）：快速讓網路大致匹配 LES（數百 → 千步 Adam）。
   - Stage 1 — **joint training**（加入 (\mathcal{L}\_{\text{PDE}}) 與 BC loss）：用 Adam，並啟用 adaptive weighting（下一節）。
   - Stage 2 — **精調**（可選）：嘗試 L-BFGS 或更小 LR 的 Adam 微調 PDE 殘差（視模型大小與記憶體）。

3. **Adaptive loss weighting**

   - 使用梯度幅值平衡（GradNorm）或動態調整 (\lambda) 使 data / PDE 梯度量級近似，避免單一項主導。
   - 可每 N steps 更新一次權重比例。

4. **Learning rate / optim**

   - Adam 起始 (1\times10^{-3}) → 降至 (1\times10^{-4})；最後 L-BFGS 若可用。
   - Gradient clipping for stability。

5. **Batching & sampling**

   - Data points: use all measured points per batch if not too many；collocation:採 Sobol 或均勻隨機抽樣，多數在 domain interior 且增加近壁密度。
   - Collocation 點數需比 data 大（例如 5–10×）以強化 PDE constraint。

---

## 6. 穩定性策略（關鍵，因為你是迭代 solver）

- **relaxation factor (\alpha)**：限制每步更新幅度，推薦初期 (\alpha\in[0.1,0.5])，並以 validation 残差自適應縮小。
- **CFL-like 檢查**：為避免一次更新產生非物理瞬變，可估算局部 CFL（基於 (|\Delta U|) 與 grid spacing）並拒絕過大更新或減小 (\alpha)。
- **限幅損失**（見正則項）與 gradient clipping。
- **監控 divergence 與 PDE residual 分布**：若 residual 突增，回退上一步（rollback）或將 (\alpha) 減半。
- **模型輸出 activation**：最後層用 tanh/scaled linear，且依據無量綱尺度縮放輸出。

---

## 7. 驗證指標與監控

- Data (L_2) error on hold-out LES points
- (L*2) of divergence: (| \nabla\cdot\mathbf{U}|*{2})
- Momentum residual norm: (|R*{\text{mom}}|*{2})
- Stability 指標：最大 |ΔU|、number of rollback events
- Physical diagnostics（post-analysis）：mean speed profiles, vorticity maps, energy spectra（若可）

---

## 8. 消融實驗（必做，檢驗可行性）

- **A1**: 無 auxiliary losses（不使用 TKE/Tuw） vs 使用它們 → 看 closure-identifiability 改善幅度。
- **A2**: 顯式 (\hat p) 輸出 vs 流函數表述（2D） → 哪一方案更穩定/準確。
- **A3**: (\mathbf{G}(x))（learn residual field）vs learn (\nu_t(x))（eddy viscosity） → 哪一方法更可解釋、泛化性較好。
- **A4**: 不同 (\alpha)（relaxation）與限幅損失設定對迭代收斂性的影響。
- **A5**: Collocation 點數密度、near-wall 加密 vs 均勻 → 對結果的影響。

---

## 9. Pseudocode（訓練 + 迭代使用）

```
# Preprocess: normalize, build D, C, B
# Stage0: data-only pretrain
for it in range(pretrain_steps):
    pred = NN(U_t, x_batch, BC)
    loss = L_data(pred) + L_aux(pred)
    optimizer.step(loss)

# Joint training
for epoch in range(epochs):
    # sample batches
    x_d = sample(D); x_c = sample(C); x_b = sample(B)
    pred_delta = NN(U_t, x_c, BC)         # network predicts ΔU at collocation
    U_new = U_t + alpha * pred_delta
    compute L_data on x_d using U_new
    compute L_PDE on x_c using U_new (and p if predicted)
    compute L_BC on x_b
    loss = weighted sum
    optimizer.zero_grad(); loss.backward(); optimizer.step()
    # optional: update lambda weights via GradNorm
```

註：實際運行時，`U_t` 可以是 patch-based 或 global field；若 memory 限制，用 patch-training 與 overlap-aggregation。

---

## 10. 超參數建議（起始值，可調）

- collocation points: (N_c \approx 5\times N_d)
- batch size (data): 128–1024 依資料量
- Adam LR: (1e-3) → (1e-4)
- relaxation (\alpha): 0.2（初始）
- (\Delta*{\max}) （限幅）: (0.5,U*{ref})（視場量級調整）
- loss weights: start (\lambda*{data}=1.0,\ \lambda*{PDE}=0.1,\ \lambda*{BC}=1.0,\ \lambda*{aux}=0.5)，用 GradNorm 調整

---

## 11. 最後的懷疑與建議（我會反覆檢驗這點）

- 單層 slice 給出的 TKE 和 Tuw 對辨識垂直導數有限度，**不要把 closure 的物理解釋當成已證實事實**，一定要做合成實驗（用已知 (\nu_t) 的 RANS 解做 test）來驗證 pipeline。
- 初期建議先做小尺度合成實驗（2D 均勻場、多種 BC noise）驗證迭代穩定性，再換 LES 資料。
- 若你要我，我可以直接把上面損失與訓練流程 **轉成可跑的 PyTorch skeleton**（不含具體 CNN/RNN 結構，但含 autograd 計算 PDE residual、投影/流函數選項、adaptive weighting 與 rollback 機制）。你要我直接寫程式嗎？（我會直接給出可執行的範例）

幫我讀取 C:\Users\GAI\Desktop\Scott\NCA_Research\E4_PI_NCA\Discussion\20250929_PINN 設計.md 檔案並且重新整理梳理筆記架構，並且整理重新書寫檔案 並且使用$$ 符號修改數學部分顯示  
新增寫一份完整的 md 檔案

#

# CFD Solver — PINN 設計規格（Urban, pedestrian-level, 1.75m）

## 前提假設（已採用，若不符請對應修改）

- 單層 slice（$z=1.75,$m），steady-like time-averaged 資料可作為監督。
- 目標為「迭代式 solver 的單步更新器」：每次輸出 $\Delta$ field，更新場值 $\phi^{t+1}=\phi^t+\Delta\phi$。
- 中間步允許物理偏差，但希望趨向物理一致（弱約束），最終結果需物理解。
- 因為只有一層切片，垂直導數/雷諾項需用 learnable closure 或 supervised surrogate（例如直接預測 $T_{uw}$ 或 $\nu_t$）。（參見風險：[[vertical_gap]]）

---

# 目標（Goal）

- 輸入：當前場 $\phi^t$（在 slice 上）、幾何 mask、BC（已修復或原始）、pedestrian-level data (`uped`, `vped`, `Uped`, `TKEped`, `Tuwped`）在 domain/BC 上。
- 輸出：$\Delta \mathbf{U}=(\Delta u,\Delta v)$（必要時可同時輸出 $\Delta p$, $\Delta k$, $\Delta T_{uw}$ 或 closure 欄位）。
- 要求：更新後場在經若干迭代後能在數據與物理殘差間達成平衡；每步更新需受穩定性約束（避免爆炸）。

---

# I. Input / Output 定義（精確）

**Inputs（per step）**

- $(x,y)$ coordinates for collocation/data points (slice $z=1.75$).
- geometry mask / building footprint (binary or signed distance).
- BC info at domain boundary (inlet profile etc.).
- current field $\phi^t$: e.g. $u^t(x,y), v^t(x,y), p^t(x,y)$（若無 $p^t$ 可視為 0 初始或隱變量）。
- supervised measurements on slice: `uped`, `vped`, `Uped`, `TKEped`, `Tuwped` (可部分缺失)。

**Outputs**

- Primary: $\Delta u(x,y), \Delta v(x,y)$ → 更新 $\mathbf{U}^{t+1}=\mathbf{U}^{t}+\Delta\mathbf{U}$.
- Optional: $\Delta p, \Delta k, \Delta T_{uw}$ 或 closure fields (例如 $\nu_t(x,y)$ or $G_u,G_v$)。
- Diagnostics: per-step PDE residual map, divergence map, predicted $\widehat{T_{uw}}$ or $\widehat{k}$ if modeled.

---

# II. 模型要學哪些「物件」（設計選項與建議）

（要在實作前決定；我推薦 **A or C**）

A. **學習 $\Delta\mathbf{U}$ + 直接 supervised $\widehat{T_{uw}},\widehat{k}$**（推薦）

- NN 輸出 $\Delta u,\Delta v$，另輸出 $\widehat{T_{uw}}$ 與 $\widehat{k}$ 作為監督/closure。
- 優點：直接利用 LES 的 `Tuwped`、`TKEped` 作監督；closure 更可解釋。

B. **學習 $\Delta\mathbf{U}$ 並同時 learnable closure $\nu_t$ 或 $G$**

- NN 輸出 $\Delta\mathbf{U}$ 及 $\nu_t$ 或 $G_u,G_v$（aggregation of missing vertical terms）。
- 優點：更物理解釋性（$\nu_t\ge0$），但需設置物理關係（例如 $\tau_{uv}\approx -\nu_t\partial_z \bar u$ 的近似）。

C. **純 data-driven $\Delta\mathbf{U}$（但有弱 PDE penalty）**

- NN 只輸出 $\Delta\mathbf{U}$，closure 隱含在輸出中。
- 風險：可能過擬合或不可解釋。僅建議作 baseline。

---

# III. 損失函數（Loss） — 核心（逐項列出並用 LaTeX）

整體損失：
[
\mathcal{L} = \lambda_{\text{data}}\mathcal{L}*{\text{data}} + \lambda*{\text{PDE}}\mathcal{L}*{\text{PDE}} + \lambda*{\text{BC}}\mathcal{L}*{\text{BC}} + \lambda*{\text{stab}}\mathcal{L}*{\text{stab}} + \lambda*{\text{reg}}\mathcal{L}_{\text{reg}}.
]

1. **資料損失（Data loss）**：在有 LES 觀測點 $D$（slice 上），讓更新後場接近觀測（或直接對輸出 closure 監督）
   [
   \mathcal{L}*{\text{data}} = \frac{1}{|D|}\sum*{x\in D}\Big(\alpha_u|\mathbf{U}^{t+1}(x)-\mathbf{U}^{LES}(x)|^2 + \alpha_{U}\big(,U^{t+1}(x)-Uped^{LES}(x)\big)^2\Big)
   ]
   若同時預測 $\widehat{k},\widehat{T_{uw}}$，加入
   [
   +\ \alpha_k|k^{pred}-TKEped^{LES}|^2 + \alpha_{Tuw}|\widehat{T_{uw}}-Tuwped^{LES}|^2.
   ]

2. **PDE 殘差（Physics loss）**：在 collocation 集合 $C$ 上對更新後場計算弱殘差（低權重）

- 動量殘差（2D slice form with closure term $G$ if needed）：
  [
  \mathcal{R}*{mom} = \rho(\mathbf{U}^{t+1}\cdot\nabla)\mathbf{U}^{t+1} + \nabla p^{t+1} - \mu \nabla^2 \mathbf{U}^{t+1} - \mathbf{C}*{closure},
  ]
  其中 $\mathbf{C}_{closure}$ 表示 learnable closure（若採用）。
- divergence 殘差：
  [
  \mathcal{L}*{\text{PDE}} = \frac{1}{|C|}\sum*{x\in C}\Big(|\mathcal{R}*{mom}(x)|^2 + \gamma*{div}|\nabla\cdot\mathbf{U}^{t+1}(x)|^2\Big).
  ]

> 註：$\gamma_{div}$ 可大於 1 以側重不可壓縮性。

3. **邊界條件損失（BC loss）**：在邊界點集合 $B$ 上
   [
   \mathcal{L}*{\text{BC}} = \frac{1}{|B|}\sum*{x\in B}|\mathcal{B}(\mathbf{U}^{t+1},p^{t+1})(x) - BC_{target}(x)|^2.
   ]
   （若 BC 是修復後的，則以修復結果為 target。）

4. **穩定性/更新幅度限制（Stability / step-size）**：避免 $\Delta$ 過大導致發散
   [
   \mathcal{L}*{\text{stab}} = \frac{1}{|D|}\sum*{x}\mathrm{ReLU}\big(|\Delta \mathbf{U}(x)| - \delta_{\max}\big)^2
   ]
   或更平滑的 $L^2$ 平衡：
   [
   \mathcal{L}*{\text{stab}} = |\Delta \mathbf{U}|*{2}^2 \cdot w_{\text{step}}.
   ]
   （$\delta_{\max}$ 為允許每步最大改變量）

5. **正則化（Reg）**：平滑 closure、限制物理量範圍（例如 $k\ge0,\ \nu_t\ge0$）
   [
   \mathcal{L}_{\text{reg}} = \beta|\nabla \widehat{k}|^2 + \beta_2 |\mathrm{ReLU}(-\widehat{k})|^2 + \ldots
   ]

---

# IV. 權重與訓練策略（務實配方）

- 初期建議權重（可依資料比例粗調）：

  - $\lambda_{\text{data}}=1.0$（data-first pretrain）
  - $\lambda_{\text{PDE}}=0.01\sim0.1$（弱物理約束）
  - $\lambda_{\text{BC}}=1.0$（若 BC 為可靠 target）
  - $\lambda_{\text{stab}}=0.1$，$\lambda_{\text{reg}}=1e-4$

- 訓練 schedule（推薦）：

  1. **Data-only pretrain**（只最小化 $\mathcal{L}*{\text{data}}+\mathcal{L}*{\text{stab}}+\mathcal{L}_{\text{reg}}$）幾百–數千步（快速收斂到可用初始迭代器）。
  2. **Joint training**（加入 $\mathcal{L}*{\text{PDE}},\mathcal{L}*{\text{BC}}$）用 Adam 1e-3 → 1e-4，必要時再用 L-BFGS 做精調（若模型與記憶體允許）。
  3. **自適應權重**：監控各 loss 的梯度範圍，若 data loss 過主導則降低 $\lambda_{\text{data}}$ 或用 GradNorm / uncertainty weighting 自動調整。

- Optimizers：Adam for bulk training; optional L-BFGS for fine-tune. 使用 gradient clipping（norm 1–5）防止爆炸。

---

# V. Collocation / Sampling / Batching

- Data points $D$：使用 LES sample locations（可能不完整）。
- Collocation points $C$：均勻或 Sobol 取樣整個 domain（包含 boundary region 的加密 sampling）。
- Batch strategy：混合 batch（每步同時取 data batch 與 collocation batch），例如 $N_{data}=256$, $N_{col}=1024$.
- 若 domain 複雜，使用 geometry-aware sampling（more points near building edges、wake regions）。

---

# VI. 迭代與推理時行為（Inference loop）

1. 每迭代 step t：輸入 $\phi^t$ 與 BC（若 BC 需修補則跳過 solver）。
2. NN 輸出 $\Delta\mathbf{U}^t$，計算 $\mathbf{U}^{t+1}=\mathbf{U}^t+\Delta\mathbf{U}^t$。
3. 若 $||\Delta\mathbf{U}^t||*\infty < \epsilon*{\Delta}$ 或 PDE residual $<\epsilon_{\text{res}}$ → 停止迭代（converged）。
4. 否則回到下一步。

- 建議最大迭代數限制以避免無窮 loop。
- 記錄每步的 residual 與 data error，供後續 adaptive weighting 調整。

---

# VII. 驗證指標（必監控）

- Data $L^2$ error vs LES on holdout points.
- Divergence metric: $|\nabla\cdot\mathbf{U}|_{L^2}$.
- Momentum residual norm: $|\mathcal{R}*{mom}|*{L^2}$.
- Closure plausibility：$\widehat{k}\ge0$、$\widehat{T_{uw}}$ 在物理範圍內、$\nu_t$ 正值且在壁面趨近 0（視你採用何種 closure）。
- Iterative stability：每步 $|\Delta\mathbf{U}|$ 的衰減趨勢。

---

# VIII. 實務技巧與風險提醒（懷疑式建議）

- **數值尺度化（不可忽略）**：把長度、速度、時間 non-dimensionalize（$U_{ref}, L_{ref}$），否則 PDE 殘差和 data loss 會有極不平衡的量級。
- **單層切片的不可辨識性**：若只用 slice，上述 closure 須非常謹慎；最好監督 $\widehat{T_{uw}}$ 或 $\widehat{k}$（LES 提供）來降低不確定性。
- **避免一次 step 做過大改動**：用 $\mathcal{L}_{\text{stab}}$ 或直接對 output 使用 tanh-scaling 與 step-size 限制。
- **邊界行為**：wall region 必須有更細緻的 sampling 與更強的 BC loss。
- **自適應 $\lambda$ 策略很重要**：若 PDE loss 與 data loss 差很多，模型會只記 data。推薦實作 GradNorm 或簡單的階段式權重更新。
- **不確定性評估**：用 ensemble runs 或 MC dropout 獲得解的不確定性，特別在 BC 有缺失情況下。

---

# IX. 建議的初始超參（可作起點）

- $N_{data_batch}=256$, $N_{col_batch}=1024$
- Adam lr = $1\mathrm{e}{-3}$ (pretrain), then $1\mathrm{e}{-4}$ (joint)
- $\lambda_{\text{data}}=1.0$, $\lambda_{\text{PDE}}=0.05$, $\gamma_{div}=5.0$, $\lambda_{BC}=1.0$, $\lambda_{\text{stab}}=0.1$
- $\delta_{\max}$（每步最大變化）≈ $0.1\cdot U_{ref}$（視場尺度）
- max iterations per inference run: 50–200（依情況）

---

# X. 最後 — 可立即採取的實驗計畫（3-step）

1. **Baseline（data-only iterative updater）**：訓練 NN 輸出 $\Delta\mathbf{U}$，僅最小化 data loss + stability。評估能否復原 LES。
2. **加入弱 PDE**：在 joint training 中加入 PDE 殘差（小權重），觀察 divergence 與 momentum residual 變化。
3. **加入 closure supervision**：同時讓 NN 預測 $\widehat{T_{uw}}$ 和 $\widehat{k}$（若 LES 可用），改進物理解釋性與泛化。

---

如果你要，我可以把上述規格直接轉成 **PyTorch training skeleton**（包含自動微分求 PDE 殘差、loss 組合、訓練 loop 與 inference loop），或把損失與 training schedule 換成 YAML/task list 讓工程師直接實作。你選哪個？
